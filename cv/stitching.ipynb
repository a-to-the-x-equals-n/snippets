{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Compose`\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Compose Multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image\n",
    "\n",
    "F = TypeVar(\"F\", float, np.floating)\n",
    "\n",
    "def _compose_all(frame: Image.Image, images: Sequence[Image.Image], transformations: Sequence[NDArray[F]]) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Composites multiple images onto a base frame using affine transformations.\n",
    "\n",
    "    The function iteratively applies 3x3 affine transformations to overlay images onto \n",
    "    the background frame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    frame : Image.Image\n",
    "        The base image onto which the other images will be composited.\n",
    "    images : Sequence[Image.Image]\n",
    "        A sequence of PIL images to be transformed and composited onto the frame.\n",
    "    transformations : Sequence[NDArray[F]]\n",
    "        A sequence of (3,3) NumPy arrays representing affine transformations for each image.\n",
    "        Each transformation matrix should be invertible.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Image.Image\n",
    "        The final composed image with all transformed images applied.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The transformation matrices must be invertible.\n",
    "    - Uses `Image.AFFINE` transformation mode with `Image.BICUBIC` interpolation.\n",
    "    - The compositing order follows the sequence order.\n",
    "\n",
    "    Complexity:\n",
    "    -----------\n",
    "    - Matrix inversion: O(1) per image.\n",
    "    - Image transformation: O(W Ã— H) where W and H are image dimensions.\n",
    "    - Composition: O(N) for N images.\n",
    "    \"\"\"\n",
    "\n",
    "    width, height = frame.size\n",
    "\n",
    "    for image, transformation in zip(images, transformations):\n",
    "        # Ensure transformation is a NumPy array\n",
    "        transformation = np.asarray(transformation, dtype = np.float32)\n",
    "\n",
    "        # Validate shape\n",
    "        if transformation.shape != (3, 3):\n",
    "            raise ValueError(\"Each transformation matrix must have shape (3, 3).\")\n",
    "\n",
    "        # Compute inverse transformation matrix (needed for PIL)\n",
    "        try:\n",
    "            t = np.linalg.inv(transformation)\n",
    "        except np.linalg.LinAlgError:\n",
    "            raise ValueError(\"Transformation matrix must be invertible.\")\n",
    "\n",
    "        # Extract affine parameters\n",
    "        a, b, c = t[0, 0], t[0, 1], t[0, 2]\n",
    "        d, e, f = t[1, 0], t[1, 1], t[1, 2]\n",
    "\n",
    "        # Apply affine transformation to the image\n",
    "        transformed_image = image.transform(\n",
    "            (width, height), Image.AFFINE, (a, b, c, d, e, f), Image.BICUBIC\n",
    "        )\n",
    "\n",
    "        # Create binary mask based on non-zero pixels\n",
    "        im_array = np.asarray(transformed_image)\n",
    "        mask_array = 255 * (np.sum(im_array, axis=-1) > 0)  # Non-zero pixels as mask\n",
    "        mask = Image.fromarray(mask_array.astype(np.uint8)).convert(\"1\")\n",
    "\n",
    "        # Composite the transformed image onto the frame\n",
    "        frame = Image.composite(transformed_image, frame, mask)\n",
    "\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Compose [unstable]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(source: np.ndarray, target: np.ndarray, transform: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    # dimension grabbing\n",
    "    h, w, _ = source.shape\n",
    "    target_h, target_w, _ = target.shape\n",
    "\n",
    "    result = np.copy(target)\n",
    "    \n",
    "    # invert transformation matrix for backward mapping\n",
    "    transform_inv = np.linalg.inv(transform)\n",
    "\n",
    "    for y in range(target_h):\n",
    "        for x in range(target_w):\n",
    "\n",
    "            # homogenize\n",
    "            # aka add one dimension\n",
    "            xy_target_homogenized = np.array([x, y, 1]) # (x, y) to (x, y, 1)\n",
    "\n",
    "            # backward mapping\n",
    "            xy_backward_map = transform_inv @ xy_target_homogenized.T # convert to column vector by using \"T\"\n",
    "            xy_backward_map = np.array(xy_backward_map).flatten() # convert to 1d array\n",
    "\n",
    "            # homogenous divide to get source coordinates\n",
    "            x_source = xy_backward_map[0] / xy_backward_map[2]\n",
    "            y_source  = xy_backward_map[1] / xy_backward_map[2]\n",
    "\n",
    "            # just another classic boundary check\n",
    "            if 0 <= x_source < w and 0 <= y_source < h: # we can siimply discard anything not in bounds\n",
    "\n",
    "                # NOTE FOR FUTURE LOGAN\n",
    "                ''' \n",
    "                    We multiplied the inverse of the transformation matrix with a homogenized x and y from the target image.\n",
    "                    \n",
    "                    That process is a \"map\" to find the exact x and y coords from a source.\n",
    "                    \n",
    "                        Now that we've found the position of WHERE the source image's x and y are...\n",
    "\n",
    "                        ...we interpolate them to grab an estimated pixel color from that location.\n",
    "                    \n",
    "                    Once we have the pixel, we set the current x and y coordinate of the target to that pixel.\n",
    "                '''\n",
    "                result[y, x] = _interpolate(source, x_source, y_source)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Stitching`\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1. SIFT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Key Points`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "left = cv2.imread(\"im1_left.jpg\")\n",
    "center = cv2.imread(\"im1_center.jpg\")\n",
    "right = cv2.imread(\"im1_right.jpg\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "axes[0].imshow(left[:,:,::-1])\n",
    "axes[1].imshow(center[:,:,::-1])\n",
    "axes[2].imshow(right[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeypoints(image):\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # NOTE: keypoints have an (x, y), scale, orientation, response (strength of feature), and octave (layer of the scale space where it was found)\n",
    "    # NOTE: descriptors are 128 element vector describing the neighborhood of the corresponding keypoint\n",
    "    \n",
    "    keypoints, descriptors = sift.detectAndCompute(image = gray, mask = None)\n",
    "\n",
    "    return keypoints, descriptors, gray\n",
    "\n",
    "\n",
    "\n",
    "def draw_kp(images, titles):\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "\n",
    "    for i, (image, title) in enumerate(zip(images, titles)):\n",
    "        kp, _, gray = getKeypoints(image)\n",
    "\n",
    "        canvas = cv2.drawKeypoints(\n",
    "            image = gray, \n",
    "            keypoints = kp, \n",
    "            outImage = None, \n",
    "            flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "        )\n",
    "\n",
    "        plt.subplot(1, len(images), i + 1) # 1 row, num of images as columns, current position\n",
    "        plt.imshow(canvas, cmap = 'gray')\n",
    "        plt.title(f'{title}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "images = [left, center, right]\n",
    "titles = ['left', 'center', 'right']\n",
    "\n",
    "draw_kp(images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Matches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatches(query_des, train_des, ratio = .75):\n",
    "\n",
    "    # init brute force matcher\n",
    "    bf = cv2.BFMatcher(normType = cv2.NORM_L2, crossCheck = False)\n",
    "    # find matches\n",
    "    matches = bf.knnMatch(queryDescriptors = query_des, trainDescriptors = train_des, k = 2)\n",
    "    return [ m for m, n in matches if m.distance < ratio * n.distance ]\n",
    "\n",
    "def draw_matches(query_image, kpq, train_image, kpt, matches, title):\n",
    "\n",
    "    result = cv2.drawMatches(\n",
    "        img1 = query_image, \n",
    "        keypoints1 = kpq, \n",
    "        img2 = train_image, \n",
    "        keypoints2 = kpt, \n",
    "        matches1to2 = matches, \n",
    "        outImg = None,\n",
    "        flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize = (5, 5))\n",
    "    plt.imshow(result[...,:: -1])\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# keypoints and descriptors\n",
    "kp_l, des_l, _ = getKeypoints(left)\n",
    "kp_c, des_c, _ = getKeypoints(center)\n",
    "kp_r, des_r, _ = getKeypoints(right)\n",
    "\n",
    "# Matches\n",
    "matches_lc = getMatches(des_l, des_c) # left and center matches\n",
    "matches_rc = getMatches(des_r, des_c) # center and right\n",
    "\n",
    "# show left-center\n",
    "draw_matches(left, kp_l, center, kp_c, matches_lc, 'Left-Center')\n",
    "# show center-right\n",
    "draw_matches(right, kp_r, center, kp_c, matches_rc, 'Right-Center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Point Class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point():\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Homography`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _homography(s0, s1, s2, s3, t0, t1, t2, t3):\n",
    "\n",
    "    x0s = s0.x\n",
    "    y0s = s0.y\n",
    "    x0t = t0.x\n",
    "    y0t = t0.y\n",
    "\n",
    "    x1s = s1.x\n",
    "    y1s = s1.y\n",
    "    x1t = t1.x\n",
    "    y1t = t1.y\n",
    "\n",
    "    x2s = s2.x\n",
    "    y2s = s2.y\n",
    "    x2t = t2.x\n",
    "    y2t = t2.y\n",
    "\n",
    "    x3s = s3.x\n",
    "    y3s = s3.y\n",
    "    x3t = t3.x\n",
    "    y3t = t3.y\n",
    "\n",
    "    # linear constraints betwixt source and target\n",
    "    A = np.matrix([\n",
    "            [x0s, y0s, 1, 0, 0, 0, -x0t * x0s, -x0t * y0s],\n",
    "            [0, 0, 0, x0s, y0s, 1, -y0t * x0s, -y0t * y0s],\n",
    "            [x1s, y1s, 1, 0, 0, 0, -x1t * x1s, -x1t * y1s],\n",
    "            [0, 0, 0, x1s, y1s, 1, -y1t * x1s, -y1t * y1s],\n",
    "            [x2s, y2s, 1, 0, 0, 0, -x2t * x2s, -x2t * y2s],\n",
    "            [0, 0, 0, x2s, y2s, 1, -y2t * x2s, -y2t * y2s],\n",
    "            [x3s, y3s, 1, 0, 0, 0, -x3t * x3s, -x3t * y3s],\n",
    "            [0, 0, 0, x3s, y3s, 1, -y3t * x3s, -y3t * y3s]\n",
    "        ])\n",
    "\n",
    "    # column vector of target coords\n",
    "    b = np.array([\n",
    "            [x0t],\n",
    "            [y0t],\n",
    "            [x1t],\n",
    "            [y1t],\n",
    "            [x2t],\n",
    "            [y2t],\n",
    "            [x3t],\n",
    "            [y3t]\n",
    "        ])\n",
    "    \n",
    "    try:\n",
    "        # A^-1 @ b\n",
    "        solutions = np.linalg.solve(A, b)\n",
    "        \n",
    "    except np.linalg.LinAlgError:\n",
    "        # the matrix isn't invertible\n",
    "        A += np.eye(A.shape[0]) * 1e-10 # add SMALL value to matrix to make it invertible\n",
    "        solutions = np.linalg.solve(A, b)\n",
    "\n",
    "    # homogenize the matrix\n",
    "    solutions = np.append(solutions, [[1.0]], axis = 0)\n",
    "\n",
    "    # reshape homography to 3x3 matrix\n",
    "    homography = np.reshape(solutions, (3,3))\n",
    "    \n",
    "    return homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2. Ransac`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ransac(kp_q, kp_t, matches, max = 1000, threshold = 5.0):\n",
    "    import random\n",
    "\n",
    "    best_H = None\n",
    "    max_inliers = 0\n",
    "    best_inliers = []\n",
    "\n",
    "    for _ in range(max):\n",
    "        samples = random.sample(matches, 4)\n",
    "\n",
    "        query_points = [Point(*kp_q[m.queryIdx].pt) for m in samples]\n",
    "        train_points = [Point(*kp_t[m.trainIdx].pt) for m in samples]\n",
    "\n",
    "        H = _homography(*query_points, *train_points)\n",
    "\n",
    "        inliers = []\n",
    "\n",
    "        for m in matches:\n",
    "\n",
    "            pt1 = np.array([*kp_q[m.queryIdx].pt, 1.0]) # homogenized coords\n",
    "            pt2 = np.array(kp_t[m.trainIdx].pt)\n",
    "\n",
    "            projected_pt = H @ pt1\n",
    "            projected_pt /= projected_pt[2] # normalize to grab (x, y)\n",
    "\n",
    "            # euclidean distance as the reprojection error\n",
    "            error = np.linalg.norm(pt2 - projected_pt[:2])\n",
    "\n",
    "            # check if distance is w/in threshold\n",
    "            if error < threshold:\n",
    "                inliers.append(m)\n",
    "\n",
    "            # update best homography\n",
    "            if len(inliers) > max_inliers:\n",
    "                max_inliers = len(inliers)\n",
    "                best_H = H\n",
    "                best_inliers = inliers\n",
    "            \n",
    "    return best_H, best_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Interpolate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(image: np.ndarray, x: float, y: float) -> float | int:\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    # get neighboring points\n",
    "    # clip to w/in bounds if necessary\n",
    "    x0 = max(int(np.floor(x)), 0)\n",
    "    y0 = max(int(np.floor(y)), 0)\n",
    "    x1 = min(x0 + 1, w - 1) \n",
    "    y1 = min(y0 + 1, h - 1)\n",
    "\n",
    "    # distances/weights between TL pixel and orginal pixel\n",
    "    xw = x - x0\n",
    "    yw = y - y0\n",
    "\n",
    "    # neighboring pixel values\n",
    "    p00 = image[y0, x0].astype(np.float32) # TL\n",
    "    p01 = image[y0, x1].astype(np.float32) # TR\n",
    "    p10 = image[y1, x0].astype(np.float32) # BL\n",
    "    p11 = image[y1, x1].astype(np.float32) # BR\n",
    "\n",
    "    return ( \n",
    "        p00 * (1 - xw) * (1 - yw) + \n",
    "        p01 * xw * (1 - yw) + \n",
    "        p10 * (1 - xw) * yw + \n",
    "        p11 * xw * yw\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Warp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compose(source: np.ndarray, target: np.ndarray, transform: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    # dimension grabbing\n",
    "    h, w, _ = source.shape\n",
    "    target_h, target_w, _ = target.shape\n",
    "\n",
    "    result = np.copy(target)\n",
    "    \n",
    "    # invert transformation matrix for backward mapping\n",
    "    transform_inv = np.linalg.inv(transform)\n",
    "\n",
    "    for y in range(target_h):\n",
    "        for x in range(target_w):\n",
    "            \n",
    "            # homogenize\n",
    "            # aka add one dimension\n",
    "            xy_target_homogenized = np.array([x, y, 1]) # (x, y) to (x, y, 1)\n",
    "\n",
    "            # backward mapping\n",
    "            xy_backward_map = transform_inv @ xy_target_homogenized.T # convert to column vector by using \"T\"\n",
    "            xy_backward_map = np.array(xy_backward_map).flatten() # convert to 1d array\n",
    "\n",
    "            # homogenous divide to get source coordinates\n",
    "            x_source = xy_backward_map[0] / xy_backward_map[2]\n",
    "            y_source  = xy_backward_map[1] / xy_backward_map[2]\n",
    "\n",
    "            # just another classic boundary check\n",
    "            if 0 <= x_source < w and 0 <= y_source < h: # we can siimply discard anything not in bounds\n",
    "\n",
    "                result[y, x] = _interpolate(source, x_source, y_source)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "H_left, inliers = _ransac(kp_l, kp_c, matches_lc)\n",
    "stitched_lc = compose(left, center, H_left)\n",
    "\n",
    "H_right, inliers = _ransac(kp_r, kp_c, matches_rc)\n",
    "stitched_rc = compose(right, center, H_right)\n",
    "\n",
    "_, axes = plt.subplots(nrows = 1, ncols = 2, figsize=(10, 10))\n",
    "axes[0].imshow(stitched_lc[...,:: -1]), axes[0].set_title('Left - Center')\n",
    "axes[1].imshow(stitched_rc[...,:: -1]), axes[1].set_title('Center - Right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3. Mosaic`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Canvas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image):\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    nonzero = np.nonzero(gray)\n",
    "\n",
    "    y_min, y_max = nonzero[0].min(), nonzero[0].max()\n",
    "    x_min, x_max = nonzero[1].min(), nonzero[1].max()\n",
    "\n",
    "    return image[y_min : y_max, x_min : x_max]\n",
    "\n",
    "\n",
    "def blendImages(left, center, right, H_left, H_right):\n",
    "\n",
    "    h, w, _ = center.shape\n",
    "    \n",
    "    canvas = np.zeros((3 * h, 3 * w, 3), dtype = np.uint8)\n",
    "    y = (canvas.shape[0] - h) // 2\n",
    "    x = (canvas.shape[1] - w) // 2\n",
    "\n",
    "    T = np.array([\n",
    "        [1, 0, w],\n",
    "        [0, 1, h],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    left_transform = T @ H_left\n",
    "    right_transform = T @ H_right\n",
    "\n",
    "    canvas = compose(left, np.copy(canvas), left_transform)\n",
    "    canvas[ y : y + h, x : x + w] = center\n",
    "    canvas = compose(right, np.copy(canvas), right_transform)\n",
    "\n",
    "    return crop(canvas)\n",
    "\n",
    "\n",
    "panorama = blendImages(left, center, right, H_left, H_right)\n",
    "\n",
    "plt.imshow(panorama[..., ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4. Stitch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageStitch(left, center, right):\n",
    "\n",
    "    ''' keypoints '''\n",
    "    kp_left, des_left, _ = getKeypoints(left)\n",
    "    kp_center, des_center, _ = getKeypoints(center)\n",
    "    kp_right, des_right, _ = getKeypoints(right)\n",
    "\n",
    "    ''' matches '''\n",
    "    left_center_matches = getMatches(des_left, des_center)\n",
    "    right_center_matches = getMatches(des_right, des_center)\n",
    "\n",
    "    ''' RANSAC '''\n",
    "    H_left, _ = _ransac(kp_left, kp_center, left_center_matches)\n",
    "    H_right, _ = _ransac(kp_right, kp_center, right_center_matches)\n",
    "\n",
    "    ''' stitch '''\n",
    "    panorama = blendImages(left, center, right, H_left, H_right)\n",
    "\n",
    "    return panorama\n",
    "\n",
    "\n",
    "left = cv2.imread(\"im1_left.jpg\")\n",
    "center = cv2.imread(\"im1_center.jpg\")\n",
    "right = cv2.imread(\"im1_right.jpg\")\n",
    "\n",
    "result = imageStitch(left,center,right)\n",
    "plt.imshow(result[:,:,::-1]);plt.show()\n",
    "\n",
    "\n",
    "left = cv2.imread(\"im2_left.jpg\")\n",
    "center = cv2.imread(\"im2_center.jpg\")\n",
    "right = cv2.imread(\"im2_right.jpg\")\n",
    "\n",
    "result = imageStitch(left,center,right)\n",
    "plt.imshow(result[:,:,::-1]);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
